{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-07T15:24:50.055298Z",
     "start_time": "2025-12-07T15:24:36.468562Z"
    }
   },
   "source": [
    "import joblib\n",
    "# ============================================================================\n",
    "# TELCO CUSTOMER CHURN PREDICTION - ANN WITH HYPERPARAMETER TUNING\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                           f1_score, confusion_matrix, classification_report,\n",
    "                           roc_auc_score, roc_curve)\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow/Keras for ANN\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. DATA LOADING AND INITIAL EXPLORATION\n",
    "# ============================================================================\n",
    "\n",
    "# Load the dataset (adjust path as needed)\n",
    "# Dataset available at: https://www.kaggle.com/datasets/blastchar/telco-customer-churn\n",
    "try:\n",
    "    url = \"https://raw.githubusercontent.com/Chanura04/ML-Coursework/main/dataset/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "\n",
    "    df = pd.read_csv(url)\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset file not found. Please download from Kaggle and update the path.\")\n",
    "    # Create sample data structure for reference\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "# Display basic information\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\"*50)\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\"*50)\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# ============================================================================\n",
    "# 2. DATA PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the Telco Customer Churn dataset\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original\n",
    "    data = df.copy()\n",
    "\n",
    "    # Handle TotalCharges column - convert to numeric, errors='coerce' will set invalid to NaN\n",
    "    data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')\n",
    "\n",
    "    # Check class distribution\n",
    "    print(\"\\nClass distribution (Churn):\")\n",
    "    print(data['Churn'].value_counts())\n",
    "    print(f\"\\nChurn rate: {(data['Churn'].value_counts()[1] / len(data)) * 100:.2f}%\")\n",
    "\n",
    "    # Drop customerID as it's not useful for prediction\n",
    "    if 'customerID' in data.columns:\n",
    "        data = data.drop('customerID', axis=1)\n",
    "\n",
    "    # Handle missing values\n",
    "    print(f\"\\nMissing values after conversion: {data.isnull().sum().sum()}\")\n",
    "\n",
    "    # Impute missing values in TotalCharges with median\n",
    "    if data['TotalCharges'].isnull().sum() > 0:\n",
    "        median_total_charges = data['TotalCharges'].median()\n",
    "        data['TotalCharges'] = data['TotalCharges'].fillna(median_total_charges)\n",
    "        print(f\"Imputed {data['TotalCharges'].isnull().sum()} missing values in TotalCharges\")\n",
    "\n",
    "    # Separate features and target\n",
    "    X = data.drop('Churn', axis=1)\n",
    "    y = data['Churn']\n",
    "\n",
    "    # Encode target variable\n",
    "    y = y.map({'Yes': 1, 'No': 0})\n",
    "\n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "    print(f\"\\nCategorical columns: {categorical_cols}\")\n",
    "    print(f\"Numerical columns: {numerical_cols}\")\n",
    "\n",
    "    # Handle specific binary categorical columns\n",
    "    binary_cols = ['gender', 'Partner', 'Dependents', 'PhoneService',\n",
    "                   'PaperlessBilling', 'SeniorCitizen']  # SeniorCitizen is already 0/1\n",
    "\n",
    "    # For binary columns, use Label Encoding\n",
    "    for col in binary_cols:\n",
    "        if col in categorical_cols:\n",
    "            if col == 'gender':\n",
    "                X[col] = X[col].map({'Female': 0, 'Male': 1})\n",
    "            elif col in ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling']:\n",
    "                X[col] = X[col].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "    # Update categorical columns list after encoding binary columns\n",
    "    categorical_cols = [col for col in categorical_cols if col not in binary_cols]\n",
    "\n",
    "    # Create preprocessing pipelines\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    # Combine preprocessing steps\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ])\n",
    "\n",
    "    return X, y, preprocessor, categorical_cols, numerical_cols\n",
    "\n",
    "# Preprocess the data\n",
    "if not df.empty:\n",
    "    X, y, preprocessor, categorical_cols, numerical_cols = preprocess_data(df)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "    print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "    # Apply preprocessing\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    # Get feature names after one-hot encoding\n",
    "    feature_names = numerical_cols.copy()\n",
    "    if categorical_cols:\n",
    "        ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "        categorical_features = ohe.get_feature_names_out(categorical_cols)\n",
    "        feature_names.extend(categorical_features)\n",
    "\n",
    "    print(f\"\\nProcessed feature shape: {X_train_processed.shape}\")\n",
    "    print(f\"Number of features: {len(feature_names)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. ANN MODEL BUILDING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def create_ann_model(learning_rate=0.001,\n",
    "                     hidden_layers=2,\n",
    "                     neurons_per_layer=64,\n",
    "                     dropout_rate=0.3,\n",
    "                     activation='relu',\n",
    "                     optimizer='adam',\n",
    "                     l1_reg=0.001,\n",
    "                     l2_reg=0.001):\n",
    "    \"\"\"\n",
    "    Create a configurable ANN model for binary classification\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(Dense(neurons_per_layer,\n",
    "                    input_dim=X_train_processed.shape[1],\n",
    "                    activation=activation,\n",
    "                    kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Hidden layers\n",
    "    for i in range(hidden_layers - 1):\n",
    "        model.add(Dense(neurons_per_layer,\n",
    "                        activation=activation,\n",
    "                        kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer (binary classification)\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile model\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    else:\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy',\n",
    "                 tf.keras.metrics.Precision(name='precision'),\n",
    "                 tf.keras.metrics.Recall(name='recall'),\n",
    "                 tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# ============================================================================\n",
    "# 4. HYPERPARAMETER TUNING USING GRID SEARCH\n",
    "# ============================================================================\n",
    "\n",
    "def perform_hyperparameter_tuning(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for the ANN model\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"HYPERPARAMETER TUNING - ANN MODEL\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Create KerasClassifier wrapper for scikit-learn compatibility\n",
    "    ann_model = KerasClassifier(\n",
    "        build_fn=create_ann_model,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Define hyperparameter grid\n",
    "    param_grid = {\n",
    "        'batch_size': [32, 64, 128],\n",
    "        'epochs': [50, 100],\n",
    "        'learning_rate': [0.001, 0.01, 0.0001],\n",
    "        'hidden_layers': [1, 2, 3],\n",
    "        'neurons_per_layer': [32, 64, 128],\n",
    "        'dropout_rate': [0.2, 0.3, 0.5],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'optimizer': ['adam', 'rmsprop'],\n",
    "        'l1_reg': [0.0, 0.001, 0.01],\n",
    "        'l2_reg': [0.0, 0.001, 0.01]\n",
    "    }\n",
    "\n",
    "    # Note: Full grid search can be computationally expensive\n",
    "    # For demonstration, we'll use a smaller subset or RandomizedSearchCV\n",
    "\n",
    "    # Alternatively, use RandomizedSearchCV for faster tuning\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "    # Create a smaller parameter distribution for randomized search\n",
    "    param_dist = {\n",
    "        'batch_size': [32, 64],\n",
    "        'epochs': [50, 80],\n",
    "        'learning_rate': [0.001, 0.01],\n",
    "        'hidden_layers': [2, 3],\n",
    "        'neurons_per_layer': [64, 128],\n",
    "        'dropout_rate': [0.3, 0.5],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'optimizer': ['adam'],\n",
    "        'l1_reg': [0.0, 0.001],\n",
    "        'l2_reg': [0.001, 0.01]\n",
    "    }\n",
    "\n",
    "    # Create callbacks for early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=0.00001,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Manual hyperparameter tuning approach (more practical)\n",
    "    print(\"\\nManual Hyperparameter Tuning Results:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Test different configurations manually\n",
    "    configurations = [\n",
    "        {\n",
    "            'name': 'Config 1 - Basic',\n",
    "            'params': {\n",
    "                'learning_rate': 0.001,\n",
    "                'hidden_layers': 2,\n",
    "                'neurons_per_layer': 64,\n",
    "                'dropout_rate': 0.3,\n",
    "                'activation': 'relu',\n",
    "                'optimizer': 'adam',\n",
    "                'batch_size': 32,\n",
    "                'epochs': 100\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'Config 2 - Deeper',\n",
    "            'params': {\n",
    "                'learning_rate': 0.001,\n",
    "                'hidden_layers': 3,\n",
    "                'neurons_per_layer': 128,\n",
    "                'dropout_rate': 0.5,\n",
    "                'activation': 'relu',\n",
    "                'optimizer': 'adam',\n",
    "                'batch_size': 64,\n",
    "                'epochs': 100\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'Config 3 - Regularized',\n",
    "            'params': {\n",
    "                'learning_rate': 0.0001,\n",
    "                'hidden_layers': 2,\n",
    "                'neurons_per_layer': 64,\n",
    "                'dropout_rate': 0.3,\n",
    "                'activation': 'tanh',\n",
    "                'optimizer': 'adam',\n",
    "                'batch_size': 32,\n",
    "                'epochs': 150,\n",
    "                'l1_reg': 0.001,\n",
    "                'l2_reg': 0.001\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    best_config = {}\n",
    "    results = []\n",
    "\n",
    "    for config in configurations:\n",
    "        print(f\"\\nTesting {config['name']}...\")\n",
    "\n",
    "        # Create and train model\n",
    "        model = create_ann_model(**{k: v for k, v in config['params'].items()\n",
    "                                    if k in ['learning_rate', 'hidden_layers',\n",
    "                                            'neurons_per_layer', 'dropout_rate',\n",
    "                                            'activation', 'optimizer', 'l1_reg', 'l2_reg']})\n",
    "\n",
    "        history = model.fit(\n",
    "            X_train_processed, y_train,\n",
    "            validation_split=0.2,\n",
    "            batch_size=config['params']['batch_size'],\n",
    "            epochs=config['params']['epochs'],\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Evaluate on test set\n",
    "        y_pred_proba = model.predict(X_test_processed, verbose=0)\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "        results.append({\n",
    "            'config_name': config['name'],\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'auc': auc,\n",
    "            'params': config['params']\n",
    "        })\n",
    "\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  F1-Score: {f1:.4f}\")\n",
    "        print(f\"  AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "        # Update best model\n",
    "        if f1 > best_score:  # Using F1-score as selection criteria\n",
    "            best_score = f1\n",
    "            best_model = model\n",
    "            best_config = config\n",
    "\n",
    "    # Display results summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"HYPERPARAMETER TUNING SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(results_df[['config_name', 'accuracy', 'precision', 'recall', 'f1_score', 'auc']])\n",
    "\n",
    "    return best_model, best_config, results_df\n",
    "\n",
    "# ============================================================================\n",
    "# 5. TRAIN FINAL MODEL WITH BEST HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "def train_final_model(X_train, y_train, X_test, y_test, best_params):\n",
    "    \"\"\"\n",
    "    Train final ANN model with the best hyperparameters\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TRAINING FINAL ANN MODEL\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Extract best parameters\n",
    "    best_params_dict = best_params['params']\n",
    "\n",
    "    # Create final model\n",
    "    final_model = create_ann_model(\n",
    "        learning_rate=best_params_dict.get('learning_rate', 0.001),\n",
    "        hidden_layers=best_params_dict.get('hidden_layers', 2),\n",
    "        neurons_per_layer=best_params_dict.get('neurons_per_layer', 64),\n",
    "        dropout_rate=best_params_dict.get('dropout_rate', 0.3),\n",
    "        activation=best_params_dict.get('activation', 'relu'),\n",
    "        optimizer=best_params_dict.get('optimizer', 'adam'),\n",
    "        l1_reg=best_params_dict.get('l1_reg', 0.001),\n",
    "        l2_reg=best_params_dict.get('l2_reg', 0.001)\n",
    "    )\n",
    "\n",
    "    # Display model architecture\n",
    "    print(\"\\nModel Architecture:\")\n",
    "    print(\"-\" * 30)\n",
    "    final_model.summary()\n",
    "\n",
    "    # Define callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=8,\n",
    "        min_lr=0.00001,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(\"\\nTraining the model...\")\n",
    "    history = final_model.fit(\n",
    "        X_train_processed, y_train,\n",
    "        validation_split=0.2,\n",
    "        batch_size=best_params_dict.get('batch_size', 32),\n",
    "        epochs=best_params_dict.get('epochs', 100),\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return final_model, history\n",
    "\n",
    "# ============================================================================\n",
    "# 6. MODEL EVALUATION AND VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, history=None):\n",
    "    \"\"\"\n",
    "    Evaluate the trained model and create visualizations\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MODEL EVALUATION\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict(X_test_processed, verbose=0)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    print(f\"AUC-ROC:   {auc:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))\n",
    "\n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "    # 1. Training History - Loss\n",
    "    if history is not None:\n",
    "        axes[0, 0].plot(history.history['loss'], label='Training Loss')\n",
    "        axes[0, 0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "        axes[0, 0].set_title('Model Loss')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Training History - Accuracy\n",
    "    if history is not None:\n",
    "        axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        axes[0, 1].set_title('Model Accuracy')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Accuracy')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Confusion Matrix Heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['No Churn', 'Churn'],\n",
    "                yticklabels=['No Churn', 'Churn'],\n",
    "                ax=axes[0, 2])\n",
    "    axes[0, 2].set_title('Confusion Matrix')\n",
    "    axes[0, 2].set_ylabel('True Label')\n",
    "    axes[0, 2].set_xlabel('Predicted Label')\n",
    "\n",
    "    # 4. ROC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    axes[1, 0].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc:.3f})')\n",
    "    axes[1, 0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "    axes[1, 0].set_xlim([0.0, 1.0])\n",
    "    axes[1, 0].set_ylim([0.0, 1.05])\n",
    "    axes[1, 0].set_xlabel('False Positive Rate')\n",
    "    axes[1, 0].set_ylabel('True Positive Rate')\n",
    "    axes[1, 0].set_title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    axes[1, 0].legend(loc=\"lower right\")\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 5. Feature Importance (using permutation importance)\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    try:\n",
    "        # Note: This can be computationally expensive\n",
    "        # Let's use a subset for faster computation\n",
    "        if X_test_processed.shape[1] > 50:  # If too many features, sample\n",
    "            sample_indices = np.random.choice(X_test_processed.shape[1], 20, replace=False)\n",
    "            X_test_sample = X_test_processed[:, sample_indices]\n",
    "            feature_names_sample = [feature_names[i] for i in sample_indices]\n",
    "        else:\n",
    "            X_test_sample = X_test_processed\n",
    "            feature_names_sample = feature_names[:20] if len(feature_names) > 20 else feature_names\n",
    "\n",
    "        # Calculate permutation importance\n",
    "        perm_importance = permutation_importance(\n",
    "            model, X_test_sample, y_test,\n",
    "            n_repeats=5,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Get top 10 important features\n",
    "        sorted_idx = perm_importance.importances_mean.argsort()[-10:]\n",
    "        axes[1, 1].barh(range(len(sorted_idx)),\n",
    "                       perm_importance.importances_mean[sorted_idx])\n",
    "        axes[1, 1].set_yticks(range(len(sorted_idx)))\n",
    "        if len(feature_names_sample) >= 10:\n",
    "            axes[1, 1].set_yticklabels([feature_names_sample[i] for i in sorted_idx])\n",
    "        axes[1, 1].set_xlabel('Permutation Importance')\n",
    "        axes[1, 1].set_title('Top 10 Feature Importances')\n",
    "    except Exception as e:\n",
    "        axes[1, 1].text(0.5, 0.5, 'Feature importance\\ncalculation failed\\n\\nError: ' + str(e),\n",
    "                       ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "        axes[1, 1].set_title('Feature Importance')\n",
    "\n",
    "    # 6. Prediction Distribution\n",
    "    axes[1, 2].hist(y_pred_proba[y_test == 0], bins=30, alpha=0.5, label='No Churn', color='blue')\n",
    "    axes[1, 2].hist(y_pred_proba[y_test == 1], bins=30, alpha=0.5, label='Churn', color='red')\n",
    "    axes[1, 2].axvline(x=0.5, color='green', linestyle='--', label='Decision Threshold')\n",
    "    axes[1, 2].set_xlabel('Predicted Probability')\n",
    "    axes[1, 2].set_ylabel('Frequency')\n",
    "    axes[1, 2].set_title('Prediction Distribution by Class')\n",
    "    axes[1, 2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ann_model_evaluation.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc': auc,\n",
    "        'confusion_matrix': cm,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# 7. MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"No data loaded. Please check the dataset path.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TELCO CUSTOMER CHURN PREDICTION - ANN IMPLEMENTATION\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    best_model, best_config, tuning_results = perform_hyperparameter_tuning(\n",
    "        X_train_processed, y_train, X_test_processed, y_test\n",
    "    )\n",
    "\n",
    "    # Train final model with best parameters\n",
    "    final_model, history = train_final_model(\n",
    "        X_train_processed, y_train, X_test_processed, y_test, best_config\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluation_results = evaluate_model(final_model, X_test_processed, y_test, history)\n",
    "\n",
    "    # Save the model\n",
    "    final_model.save('telco_churn_ann_model.h5')\n",
    "    print(\"\\nModel saved as 'telco_churn_ann_model.h5'\")\n",
    "\n",
    "    # Save preprocessing pipeline\n",
    "    import joblib\n",
    "    joblib.dump(preprocessor, 'preprocessor.pkl')\n",
    "    print(\"Preprocessing pipeline saved as 'preprocessor.pkl'\")\n",
    "\n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Best Configuration: {best_config['name']}\")\n",
    "    print(f\"Best F1-Score: {evaluation_results['f1_score']:.4f}\")\n",
    "    print(f\"Best Accuracy: {evaluation_results['accuracy']:.4f}\")\n",
    "    print(f\"Best AUC-ROC: {evaluation_results['auc']:.4f}\")\n",
    "\n",
    "    # Display best hyperparameters\n",
    "    print(\"\\nBest Hyperparameters:\")\n",
    "    for key, value in best_config['params'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 8. ADDITIONAL UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def predict_new_customer(model, preprocessor, customer_data):\n",
    "    \"\"\"\n",
    "    Predict churn probability for a new customer\n",
    "    \"\"\"\n",
    "    # Convert to DataFrame\n",
    "    customer_df = pd.DataFrame([customer_data])\n",
    "\n",
    "    # Apply preprocessing\n",
    "    customer_processed = preprocessor.transform(customer_df)\n",
    "\n",
    "    # Make prediction\n",
    "    prediction_proba = model.predict(customer_processed, verbose=0)[0][0]\n",
    "    prediction = 1 if prediction_proba > 0.5 else 0\n",
    "\n",
    "    return {\n",
    "        'churn_probability': float(prediction_proba),\n",
    "        'prediction': 'Churn' if prediction == 1 else 'No Churn',\n",
    "        'confidence': float(prediction_proba if prediction == 1 else 1 - prediction_proba)\n",
    "    }\n",
    "\n",
    "# Example customer data structure\n",
    "example_customer = {\n",
    "    'gender': 'Female',\n",
    "    'SeniorCitizen': 0,\n",
    "    'Partner': 'Yes',\n",
    "    'Dependents': 'No',\n",
    "    'tenure': 12,\n",
    "    'PhoneService': 'Yes',\n",
    "    'MultipleLines': 'No',\n",
    "    'InternetService': 'DSL',\n",
    "    'OnlineSecurity': 'No',\n",
    "    'OnlineBackup': 'Yes',\n",
    "    'DeviceProtection': 'No',\n",
    "    'TechSupport': 'No',\n",
    "    'StreamingTV': 'No',\n",
    "    'StreamingMovies': 'No',\n",
    "    'Contract': 'Month-to-month',\n",
    "    'PaperlessBilling': 'Yes',\n",
    "    'PaymentMethod': 'Electronic check',\n",
    "    'MonthlyCharges': 65.5,\n",
    "    'TotalCharges': 786.0\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTE MAIN FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    # Example of using the trained model for prediction\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXAMPLE PREDICTION FOR NEW CUSTOMER\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Load the saved model and preprocessor\n",
    "    try:\n",
    "        loaded_model = keras.models.load_model('telco_churn_ann_model.h5')\n",
    "        loaded_preprocessor = joblib.load('preprocessor.pkl')\n",
    "\n",
    "        prediction_result = predict_new_customer(\n",
    "            loaded_model, loaded_preprocessor, example_customer\n",
    "        )\n",
    "\n",
    "        print(f\"Churn Probability: {prediction_result['churn_probability']:.4f}\")\n",
    "        print(f\"Prediction: {prediction_result['prediction']}\")\n",
    "        print(f\"Confidence: {prediction_result['confidence']:.4f}\")\n",
    "    except:\n",
    "        print(\"Model files not found. Run main() first to train and save the model.\")"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras.wrappers.scikit_learn'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 27\u001B[39m\n\u001B[32m     25\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mkeras\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01moptimizers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Adam, RMSprop, SGD\n\u001B[32m     26\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mkeras\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcallbacks\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m EarlyStopping, ReduceLROnPlateau\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mkeras\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mwrappers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mscikit_learn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m KerasClassifier\n\u001B[32m     28\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mkeras\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mregularizers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m l1, l2, l1_l2\n\u001B[32m     30\u001B[39m \u001B[38;5;66;03m# Set random seeds for reproducibility\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'tensorflow.keras.wrappers.scikit_learn'"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
